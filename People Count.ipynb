{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash  \n",
    "# pip install pillow #Requirement already satisfied: pillow in /root/opt/anaconda3/lib/python3.5/site-packages\n",
    "# apt-get install libsm6 libxrender1 libfontconfig1\n",
    "\n",
    "# pip install matplotlib\n",
    "# gsutil cp gs://accelerometer-d41e2.appspot.com/accelerometer-d41e2-firebase-adminsdk-h782h-b747ff5f64.json /home/sshfile.json\n",
    "# gsutil cp gs://dirversdatatest/Testing-dc8f6bcff8e2.json /home/sshfiletest.json\n",
    "# gsutil cp gs://visuallab/mall_dataset/perspective_roi.mat /home/vis/perspective_roi.mat \n",
    "# gsutil cp gs://visuallab/mall_dataset/mall_gt.mat /home/vis/mall_gt.mat \n",
    "# gsutil cp gs://visuallab/mall_dataset/mall_feat.mat /home/vis/mall_feat.mat \n",
    "# gsutil cp gs://visuallab/mall_dataset/frames/*.jpg /home/vis/\n",
    "\n",
    "# gsutil cp gs://visuallab/mall_dataset/perspective_roi.mat /home/vis/perspective_roi.mat \n",
    "# gsutil cp gs://visuallab/mall_dataset/mall_gt.mat /home/vis/mall_gt.mat \n",
    "# gsutil cp gs://visuallab/mall_dataset/mall_feat.mat /home/mayakurtser-putty/matlab_mall/mall_feat.mat \n",
    "# conda update mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "plt.use('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import pylab as pltl\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roi = scipy.io.loadmat('/home/vis/perspective_roi.mat')\n",
    "#mall_gt = scipy.io.loadmat('/home/vis/mall_gt.mat')\n",
    "#mall_feat = scipy.io.loadmat('/home/vis/mall_feat.mat')\n",
    "\n",
    "## Replace the above paths with this: ##\n",
    "# /home/vis/ => /home/mayakurtser-putty/matlab_mall/\n",
    "# importing the data and convert it from a matlab object into a dictionary\n",
    "# store the files in the home folder of the jupiter notebook\n",
    "folder = os.getcwd()\n",
    "# print(folder)\n",
    "# print(os.path)\n",
    "roi = scipy.io.loadmat(folder+'/mall_dataset/perspective_roi.mat')\n",
    "mall_gt = scipy.io.loadmat(folder+'/mall_dataset/mall_gt.mat')\n",
    "mall_feat = scipy.io.loadmat(folder+'/mall_dataset/mall_feat.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the data in more comfortable way\n",
    "pictures = mall_gt['frame'][0]\n",
    "pmap = roi['pMapN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterates all the heads of an image and for all the heads calculate the gaussian \n",
    "# pdf in every pixel in the square that is in the length of 2*3.5*std which center\n",
    "# is the head coordinates\n",
    "\n",
    "def calcHeads(z,matrix,hight,width):\n",
    "    \n",
    "    mekadem = 5 #this mekadem been choosed ampiracly by trail and error in the article the mekadem was 0.2 but it was too small\n",
    "    \n",
    "    mekadem_body = 2.5 #this mekadem is alsow been choosed ampiracly by trail and error \n",
    "    \n",
    "    l = 3.5 * mekadem * pmap[int(480-z[1])][int(z[0])] #this is the length of half the square from the center to the edge \n",
    "    #(3.5*sigma over this \n",
    "    #value the values tends to drop to 0)\n",
    "   \n",
    "    \n",
    "#     mu_body = [z[0],max(z[1]-l-1.75 * pmap[int(480-(z[1]+l+1.75))][int(z[0])],0)]\n",
    "    \n",
    "#     k = 3.5 * mekadem_body * mekadem * pmap[int(480-mu_body[1])][int(mu_body[0])]\n",
    "    \n",
    "#     Z = multivariate_normal.pdf([z[1],z[0]],mean=[z[1],z[0]],\n",
    "#                 cov=[[mekadem*pmap[int(480-z[1])][int(z[0])],0]\n",
    "#             ,[0,mekadem*pmap[int(480-z[1])][int(z[0])]]]) \n",
    "    \n",
    "#     Z = Z + multivariate_normal.pdf([mu_body[1],mu_body[0]],mean=[mu_body[1],mu_body[0]],\n",
    "#                 cov=[[mekadem_body*mekadem*pmap[int(480-mu_body[1])][int(mu_body[0])],0]\n",
    "#                      ,[0,mekadem*pmap[int(480-mu_body[1])][int(mu_body[0])]]])\n",
    "\n",
    "    \n",
    "    for i in range (max(int(z[1]-l),0),min(int(z[1]+l),hight)):\n",
    "        for j in range (max(int(z[0]-l),0),min(int(z[0]+l),width)):\n",
    "            #this inner loop is to calculate values to the square of the head described in variable \"z\" \n",
    "            matrix[i][j] = matrix[i][j] + multivariate_normal.pdf([i,j],mean=[z[1],z[0]],\n",
    "                cov=[[mekadem*pmap[int(480-z[1])][int(z[0])],0]\n",
    "                     ,[0,mekadem*pmap[int(480-z[1])][int(z[0])]]]) \n",
    "#             / Z\n",
    "            \n",
    "#     for i in range (max(int(mu_body[1]-k),0),min(int(mu_body[1]+k),hight)):\n",
    "#         for j in range (max(int(mu_body[0]-l),0),min(int(mu_body[0]+l),width)):\n",
    "#             matrix[i][j] = matrix[i][j] + multivariate_normal.pdf([i,j],mean=[mu_body[1],mu_body[0]],\n",
    "#                 cov=[[mekadem_body*mekadem*pmap[int(480-mu_body[1])][int(mu_body[0])],0]\n",
    "#                      ,[0,mekadem*pmap[int(480-mu_body[1])][int(mu_body[0])]]]) / Z\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is calculating the density map for a speciic picture\n",
    "def dense_map(width,hight,pic):\n",
    "    print('pic:')\n",
    "    print(pic)\n",
    "    matrix = np.zeros((hight+1,width+1))#initialize the matrix with zero values\n",
    "    heads = pictures[pic][0][0][0]#save the heads in the picture\n",
    "    list(map(lambda z:calcHeads(z,matrix,hight,width),heads))#iterate all heads in pic and calculate there square pixel values\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this box is creating the PDF for all the pictures\n",
    "width = 639\n",
    "hight = 479\n",
    "# num_of_pictures = 2000 #this is the real line needs to be print because of the picture number\n",
    "num_of_pictures = 2000 #this runs for testing only\n",
    "start_train_ind = 800\n",
    "print('den_map started')\n",
    "den_map = [dense_map(width,hight,pic) for pic in range (start_train_ind,num_of_pictures)]#create a list of length num_of_pictures \n",
    "#of density maps\n",
    "print('den_map finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_patch(x0 , y0 , x1 , y1 , pic):\n",
    "    #crops the patch with cordinates x0,x1,y0,y1 from the density map of pic    \n",
    "    \n",
    "    back = [] # the patch to return\n",
    "    dense = den_map[pic] #saving the pic density map from the density maps list\n",
    "    counter = 0 #counting the rows ti know in which row to shove the columns\n",
    "    for i in range(y0,y1): #loopin over the delta y\n",
    "        \n",
    "        back.append([]) #new row\n",
    "        col = []\n",
    "        for j in range(x0,x1):#looping over the delta x\n",
    "            col.append(den_map[pic][hight-i][width-j]) #adding new pixel from the column\n",
    "        back[counter] = col #adding the entire col to the row\n",
    "        counter+=1 # next row\n",
    "        \n",
    "    return back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense(x0 , y0 , x1 , y1 , pic):\n",
    "    dim = 18 #the length of the patch after resize\n",
    "    dense = density_patch(x0 , y0 , x1 , y1 , pic)#getting the dense patch\n",
    "    back1 = scipy.ndimage.zoom(dense, dim/len(dense), order=1)#resizing the density patch to 18X18\n",
    "    return back1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(x0 , y0 , x1 , y1 , pic):\n",
    "    \n",
    "    back = 0 #heads counter\n",
    "    heads = pictures[pic][0][0][0] #gettint the list of heads in pic\n",
    "    \n",
    "    for i in heads: #itterate over all heads and add 1 to counter if the head is in delta x delta y i.e in the patch\n",
    "        if x0 <= width - i[0] and x1>=width - i[0] and y0 <=hight- i[1] and y1>=hight - i[1]:\n",
    "            back+= 1\n",
    "            \n",
    "    return back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count2(x0 , y0 , x1 , y1 , pic):\n",
    "    #countint the number of heads in the picture by integraling over the patch from the dense map\n",
    "    dense = density_patch(x0 , y0 , x1 , y1 , pic)     \n",
    "    return sum(sum(dense,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_per_pic = 80 #number of patches\n",
    "patches = [] #the patches from all the pictures (picture(0-1999),number of patch(0-79))\n",
    "patches_rgb = [] #the patches from all the pictures with the rgb value in each pixel (picture(0-1999),number of patch(0-79))\n",
    "tuples = [] # the delta x and delta y of each patches (x0,y0,x1,y1) (picture(0-1999),number of patch(0-79))\n",
    "densities = [] # the patches from all the density maps (picture(0-1999),number of patch(0-79))\n",
    "counts = [] # the counts of people in each patch by iterate heads (picture(0-1999),number of patch(0-79))\n",
    "counts2 = [] # the counts of people in each patch by integration (picture(0-1999),number of patch(0-79))\n",
    "examples = [] # the patches that has people in it (picture(0-1999),number of patch(0-79))\n",
    "\n",
    "for i in range(0,num_of_pictures):#iterate over all the pictures\n",
    "# for i in range(0,10):#iterate over all the pictures\n",
    "  \n",
    "    #add a new list to all the lists, all the patches to pic i\n",
    "    patches.append([]) \n",
    "    patches_rgb.append([])\n",
    "    tuples.append([])\n",
    "    densities.append([])\n",
    "    counts.append([])\n",
    "    counts2.append([])\n",
    "    #the path of the picture\n",
    "    im_path = folder+'/mall_dataset/frames/seq_' + str(i+1).rjust(6,'0') + '.jpg'\n",
    "\n",
    "    #import the pic and rotate it because it's upside-down\n",
    "    img = Image.open(im_path,\"r\").rotate(180)\n",
    "    \n",
    "    for j in range(0,patches_per_pic): #iterate over all the patches\n",
    "        \n",
    "        flag = True #indicates that the delta x, delta y are ligable\n",
    "        \n",
    "        while flag:\n",
    "            \n",
    "            y = random.randint(0,hight) #randomly choosing the center of a square - y coordinate\n",
    "            x = random.randint(0,width) #randomly choosing the center of a square - x coordinate\n",
    "            ranges = int(pmap[int(479-y)][x])  *  20 # the length of the square \n",
    "            l = int(ranges/2) #l is the half the length of the square the distance between the center of the square to the edge\n",
    "            \n",
    "            if x - l >= 0 and x + l < width and y - l >= 0 and y + l < hight and l!=0: #checking if the length is leagiable\n",
    "\n",
    "                tup = (x - l,y - l ,x + l,y + l) #the delta x delta y\n",
    "                patch = img.crop(tup).resize((72, 72), Image.ANTIALIAS) #crop the patch\n",
    "                patches[i].append(patch) #add to the list\n",
    "                patches_rgb[i].append(list(patch.getdata())) #change to rgb\n",
    "                tuples[i].append(tup) #add to tuples list\n",
    "                densities[i].append(get_dense(x-l,y-l,x+l,y+l,i)) #croping the patch fron the density map\n",
    "                counts[i].append(get_count(x-l,y-l,x+l,y+l,i)) #counting by heads list\n",
    "                counts2[i].append(get_count2(x-l,y-l,x+l,y+l,i)) #counting by integral\n",
    "                if counts[i][j] > 1: #saving the patches with people in it\n",
    "                    aaa = i\n",
    "                    bbb = j\n",
    "                    examples.append([i,j])\n",
    "                flag = False # the crop is legiable \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just for testing\n",
    "aaa = examples[0][0]\n",
    "bbb = examples[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints for tests\n",
    "print(aaa,bbb)\n",
    "print(patches[aaa][bbb])#checked is good\n",
    "print(tuples[aaa][bbb])#checked represent the right messure\n",
    "print(len(densities[aaa][bbb]))\n",
    "print(counts[aaa][bbb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing an example patch\n",
    "im = imshow(patches[aaa][bbb],origin=\"Upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the patch of the density map\n",
    "side = np.linspace(0,len(densities[aaa][bbb]),len(densities[aaa][bbb]))\n",
    "side1 = np.linspace(0,len(densities[aaa][bbb]),len(densities[aaa][bbb]))\n",
    "X,Y = np.meshgrid(side,side1)\n",
    "a = np.matrix(densities[aaa][bbb])\n",
    "dt = pd.DataFrame(data=a)\n",
    "\n",
    "dense2 = imshow(X=dt,interpolation='bilinear', cmap=cm.cubehelix,\n",
    "                origin='Upper',\n",
    "                vmax=abs(a).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see a picture\n",
    "im = imshow(Image.open(folder+'/mall_dataset/frames/seq_000001.jpg').rotate(180),origin='Upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see if a density map is legiable\n",
    "side = np.linspace(0,479,480)\n",
    "side1 = np.linspace(0,639,640)\n",
    "X,Y = np.meshgrid(side,side1)\n",
    "a = np.matrix(den_map[0])\n",
    "dt = pd.DataFrame(data=a)\n",
    "\n",
    "dense2 = imshow(X=ndimage.rotate(dt,180),interpolation='bilinear', cmap=cm.cubehelix,\n",
    "                origin='Upper',\n",
    "                vmax=abs(a).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see the density on top of the picture\n",
    "im = imshow(Image.open(folder+'/mall_dataset/frames/seq_000001.jpg').rotate(180))\n",
    "\n",
    "dense = imshow(X=ndimage.rotate(dt,180),interpolation='bilinear'\n",
    "               , \n",
    "               cmap=cm.cubehelix,\n",
    "                origin='Lower',\n",
    "                vmax=abs(a).max(),alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatning the input data to a list of patches - length = (2000*80)\n",
    "patches_rgb = [j for i in patches_rgb for j in i]#features\n",
    "densities = [j for i in densities for j in i]#labels 1\n",
    "counts = [j for i in counts for j in i]# labels 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing only\n",
    "print(len(patches_rgb[0][0]))\n",
    "print(len(densities[0]))\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing only\n",
    "len(densities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "train_length = int(0.7*len(patches_rgb))\n",
    "\n",
    "train_data = np.asarray(patches_rgb[0:train_length],dtype=np.float32)\n",
    "train_labels1 = np.asarray(densities[0:train_length],dtype=np.float32)\n",
    "train_labels2 = np.asarray(counts[0:train_length],dtype=np.float32)\n",
    "\n",
    "eval_data = np.asarray(patches_rgb[train_length:],dtype=np.float32)\n",
    "eval_labels = np.asarray(densities[train_length:],dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing only\n",
    "print(train_data.shape)\n",
    "print(train_labels1.shape)\n",
    "print(\"**for testing, printing one density map**\")\n",
    "print(train_labels1[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( den_map, open( \"den_map.p\", \"wb\" ) )\n",
    "pickle.dump( patches, open( \"patches.p\", \"wb\" ) )\n",
    "pickle.dump( patches_rgb, open( \"patches_rgb.p\", \"wb\" ) )\n",
    "pickle.dump( tuples, open( \"tuples.p\", \"wb\" ) )\n",
    "pickle.dump( densities, open( \"densities.p\", \"wb\" ) )\n",
    "pickle.dump( counts, open( \"counts.p\", \"wb\" ) )\n",
    "# pickle.dump( counts1, open( \"counts1.p\", \"wb\" ) )\n",
    "pickle.dump( examples, open( \"examples.p\", \"wb\" ) )\n",
    "pickle.dump( train_data, open( \"train_data.p\", \"wb\" ) )\n",
    "pickle.dump( train_labels1, open( \"train_labels1.p\", \"wb\" ) )\n",
    "pickle.dump( train_labels2, open( \"train_labels2.p\", \"wb\" ) )\n",
    "pickle.dump( eval_data, open( \"eval_data.p\", \"wb\" ) )\n",
    "pickle.dump( eval_labels, open( \"eval_labels.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def losses(estimated_dens_map,labels):\n",
    "    \n",
    "    a = tf.losses.mean_squared_error(estimated_dens_map,labels)\n",
    "#     print(a)\n",
    "#     b = tf.losses.mean_squared_error(estimated_count,labels2)\n",
    "#     print(b,labels2.get_shape(),estimated_count.get_shape())\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def losses2(estimated_dens_map,labels,estimated_count,labels2):\n",
    "    \n",
    "    a = tf.losses.mean_squared_error(estimated_dens_map,labels)\n",
    "    b = tf.losses.mean_squared_error(estimated_count,labels2)\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "#   \"\"\"Model function for CNN.\"\"\"\n",
    "#    Input Layer\n",
    "#    Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    \n",
    "    input_layer = tf.reshape(features, [-1, 72, 72, 3])\n",
    "#     labels = tf.reshape(labels,[18,18])\n",
    "\n",
    "    print(input_layer)\n",
    "#     Convolutional Layer #1\n",
    "#     Computes 32 features using a 7x7 filter with ReLU activation.\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[7, 7],\n",
    "      activation=tf.nn.relu,\n",
    "      padding=\"same\")\n",
    "\n",
    "    print(conv1)\n",
    "    \n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a 2x2 filter and stride of 2\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 32 features using a 7x7 filter.\n",
    "\n",
    "    print(pool1)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=32,\n",
    "      kernel_size=[7, 7],\n",
    "      activation=tf.nn.relu,\n",
    "      padding=\"same\")\n",
    "\n",
    "    print(conv2)\n",
    "# Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    print(pool2)\n",
    "# Convolutional Layer #3\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "\n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      activation=tf.nn.relu,\n",
    "      padding=\"same\")\n",
    "\n",
    "    print(conv3)\n",
    "    \n",
    "    # Dense Layer\n",
    "  \n",
    "    pool2_flat = tf.reshape(conv3, [-1,18 * 18 * 64])#check if this is a valid thing\n",
    "\n",
    "    print(pool2_flat)\n",
    "    \n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=1000, activation=tf.nn.relu)\n",
    "    \n",
    "    print(dense1)\n",
    "      \n",
    "# Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout1 = tf.layers.dropout(inputs=dense1, rate=0.4, training = mode == learn.ModeKeys.TRAIN)\n",
    "  \n",
    "    print(dropout1)\n",
    "    \n",
    "    dense2 = tf.layers.dense(inputs=dropout1, units=400, activation=tf.nn.relu)\n",
    "    \n",
    "    print(dense2)\n",
    "    \n",
    "    dropout2 = tf.layers.dropout(inputs=dense2, rate=0.4, training = mode == learn.ModeKeys.TRAIN)\n",
    "  \n",
    "    print(dropout2)\n",
    "    \n",
    "    #estimated_dens_map = tf.layers.dense(inputs=dense2, units=324, activation=tf.nn.relu)\n",
    "    estimated_dens_map = tf.layers.dense(inputs=dropout2, units=324, activation=tf.nn.relu)\n",
    "    print (\"***estimated_dens_map dimentions before reshaping:***\") #for debugging\n",
    "    print (estimated_dens_map.get_shape()) #for debugging  \n",
    "    estimated_dens_map = tf.reshape(estimated_dens_map,[-1,18,18])\n",
    "    \n",
    "    print(estimated_dens_map)\n",
    "    \n",
    "    print(\"labels: {}\".format(labels))\n",
    "    \n",
    "    \n",
    "#    print (\"***estimated_dens_map dimentions:***\") #for debugging\n",
    "#    print (estimated_dens_map.get_shape()) #for debugging\n",
    "#    print (\"***estimated_dens_map[100]:***\") #for debugging\n",
    "#    print (estimated_dens_map[100]) #for debugging\n",
    "#    print(labels)\n",
    "    \n",
    "#     estimated_count = tf.reshape(tf.reduce_sum(tf.layers.dense(inputs=dropout2, units=1, activation=tf.nn.relu)),[-1,1])\n",
    "# #     estimated_count = tf.reshape(tf.reduce_sum(estimated_dens_map),[-1,1])\n",
    "    \n",
    "#     print(\"estimated_count: {}\".format(estimated_count))\n",
    "\n",
    "#     labels2 = tf.reshape(tf.reduce_sum(labels),[-1,1])\n",
    "  \n",
    "#     print(\"real_count: {}\".format(labels2))\n",
    "\n",
    "    # estimated_dens_map layer\n",
    "# Input Tensor Shape: [batch_size, 1000]\n",
    "# Output Tensor Shape: [batch_size, 1000]\n",
    "#  estimated_dens_map = tf.layers.dense(inputs=dense_output_1, units=324)\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        loss = losses(estimated_dens_map,labels)\n",
    "#         loss = losses2(estimated_dens_map,labels,estimated_count,labels2)\n",
    "        \n",
    "    if mode == learn.ModeKeys.EVAL:\n",
    "        print(\"EVAL\")\n",
    "        \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        print(\"TRAIN\")\n",
    "    \n",
    "    if mode == learn.ModeKeys.INFER:\n",
    "        print(\"INFER\")\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        learning_rate=0.0001,\n",
    "        optimizer=\"SGD\")\n",
    "\n",
    "    # Generate Predictions\n",
    "#     predictions = {\n",
    "#      \"classes\": tf.argmax(\n",
    "#          input=estimated_dens_map, axis=1),\n",
    "#      \"probabilities\": tf.nn.softmax(\n",
    "#          estimated_dens_map, name=\"softmax_tensor\")\n",
    "#    }\n",
    "        \n",
    "    \n",
    "    if mode == learn.ModeKeys.INFER:\n",
    "        \n",
    "        predictions = {\"probabilities\" :estimated_dens_map}\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        predictions = {\"probabilities\" :estimated_dens_map,\"label\":labels}\n",
    "        \n",
    "        \n",
    "    print(\"end\")\n",
    "    # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode,loss=loss,predictions = predictions ,train_op=train_op)\n",
    "#      mode=mode,loss=loss ,train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "classifier = learn.Estimator(model_fn=cnn_model_fn)\n",
    "\n",
    "# monitors\n",
    "# tensors_to_log = {\"estimated_density_map\": \"Reshape_2:0\"\n",
    "#                   ,\"real_density_map\":\"output:0\"\n",
    "#                   ,\"estimate_count\":\"Reshape_3:0\"\n",
    "#                   ,\"real_count\":\"Reshape_4:0\"\n",
    "#                   ,\"features\":\"Reshape:0\"\n",
    "#                   ,\"conv1\":\"conv2d/Relu:0\"\n",
    "#                   ,\"max_pool1\":\"max_pooling2d/MaxPool:0\"\n",
    "#                   ,\"conv2\":\"conv2d_2/Relu:0\"\n",
    "#                   ,\"max_pool2\":\"max_pooling2d_2/MaxPool:0\"\n",
    "#                   ,\"conv3\":\"conv2d_3/Relu:0\"\n",
    "#                   ,\"drop1\":\"dropout/dropout/mul:0\"\n",
    "#                   ,\"drop2\":\"dropout_2/dropout/mul:0\"\n",
    "#                      ,\"MSE_a\":\"mean_squared_error/value:0\"\n",
    "# #                      ,\"MSE_b\":\"mean_squared_error_1/value:0\"\n",
    "#                  }\n",
    "\n",
    "# logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=200)\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "   eval_data,\n",
    "    eval_labels,\n",
    "    every_n_steps=100)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "classifier.fit(x=train_data,y=train_labels1,batch_size=5,monitors=[validation_monitor],max_steps=100000)\n",
    "\n",
    "# chunks = [0,2000,4000,6000,8000,10000]\n",
    "\n",
    "# for index,value in enumerate(chunks):\n",
    "    \n",
    "#     if index < len(chunks)-1:\n",
    "#         x_train = train_data[chunks[index]:chunks[index+1]]\n",
    "#         y_train = train_labels1[chunks[index]:chunks[index+1]]\n",
    "#     else:\n",
    "#         x_train = train_data[chunks[index]:]\n",
    "#         y_train = train_labels1[chunks[index]:]\n",
    "#     classifier.partial_fit(x=x_train,y=y_train,batch_size=5\n",
    "#                        ,monitors=[validation_monitor,logging_hook],steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"mse\":learn.MetricSpec(metric_fn=tf.metrics.mean_squared_error, prediction_key=\"probabilities\")\n",
    "    ,\"accuracy\":learn.MetricSpec(metric_fn=tf.contrib.metrics.streaming_precision, prediction_key=\"probabilities\")}\n",
    "\n",
    "print (\"***metrics is :***\")\n",
    "print (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "classifier\n",
    "# pickle.dump( classifier, open( \"classssifier.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_inputs():\n",
    "    x = tf.constant(eval_data)\n",
    "    y = tf.constant(eval_labels)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_inputs():\n",
    "    x = train_data\n",
    "    y = train_labels1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = classifier.evaluate(\n",
    "  x=eval_data, y=eval_labels,metrics=metrics)\n",
    "#print (eval_labels)\n",
    "#eval_results = classifier.evaluate(input_fn=get_test_inputs, steps=1)[\"accuracy\"]\n",
    "print (\"****after eval*****\")\n",
    "# print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimates = []\n",
    "estimated_sum = []\n",
    "zeros = []\n",
    "zero_2_one = []\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(0,len(eval_data)):\n",
    "#     estimated = list(classifier.predict(eval_data[i]))[0][\"probabilities\"]\n",
    "#     a = sum(sum(estimated))\n",
    "    b = sum(sum(eval_labels[i]))\n",
    "    if b > 1:\n",
    "        estimates.append(i)\n",
    "        estimated_sum.append(b)\n",
    "    else:\n",
    "        if b == 0:\n",
    "            zeros.append(b)\n",
    "        else:\n",
    "            zero_2_one.append(b)\n",
    "        \n",
    "diffs = []\n",
    "\n",
    "for i in estimates:\n",
    "    if random.uniform(0, 1) <=0.5:\n",
    "        estimated = list(classifier.predict(eval_data[i]))[0][\"probabilities\"]\n",
    "        a = sum(sum(estimated))\n",
    "        b = sum(sum(eval_labels[i]))\n",
    "        diff = b-a\n",
    "        diffs.append(diff)\n",
    "\n",
    "\n",
    "#     print(a)\n",
    "# # print(eval_results[\"accuracy\"])\n",
    "# # print(estimated)\n",
    "# # print(estimated.sum())\n",
    "# # print(eval_labels[1])\n",
    "# # print(eval_labels[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(estimates))\n",
    "print(len(estimated_sum))\n",
    "print(len(zeros))\n",
    "print(len(zero_2_one))\n",
    "print(len(diffs))\n",
    "print(np.sqrt(np.mean(np.power(diffs,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_arr = np.array(diffs)\n",
    "diffs_arr = diffs_arr[np.where(diffs_arr>-5)]\n",
    "diffs_arr = diffs_arr[np.where(diffs_arr<5)]\n",
    "print(len(diffs_arr))\n",
    "plt.pyplot.hist(diffs_arr)\n",
    "print(np.sqrt(np.mean(np.power(diffs_arr,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.sqrt(np.power([np.abs(x) for x in diffs],2)))\n",
    "diffs_arr = np.array(diffs)\n",
    "real = []\n",
    "for i in estimates:\n",
    "    b = sum(sum(eval_labels[i]))\n",
    "    real.append(b)\n",
    "real_arr = np.array(real)\n",
    "diff_per = diffs_arr/real_arr\n",
    "plt.pyplot.hist(real_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = estimates[3]\n",
    "# estimated = list(classifier.predict(eval_data[index]))[0][\"probabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing the patch of the density map\n",
    "# side = np.linspace(0,len(estimated),len(estimated))\n",
    "# side1 = np.linspace(0,len(estimated),len(estimated))\n",
    "# X,Y = np.meshgrid(side,side1)\n",
    "# a = np.matrix(estimated)\n",
    "# dt = pd.DataFrame(data=a)\n",
    "\n",
    "# dense2 = imshow(X=dt,interpolation='bilinear', cmap=cm.gist_gray,\n",
    "#                 origin='Upper',\n",
    "#                 vmax=abs(a).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# side = np.linspace(0,len(eval_labels[index]),len(eval_labels[index]))\n",
    "# side1 = np.linspace(0,len(eval_labels[index]),len(eval_labels[index]))\n",
    "# X,Y = np.meshgrid(side,side1)\n",
    "# a = np.matrix(eval_labels[index])\n",
    "# dt = pd.DataFrame(data=a)\n",
    "\n",
    "# dense2 = imshow(X=dt,interpolation='bilinear', cmap=cm.gist_gray,\n",
    "#                 origin='Upper',\n",
    "#                 vmax=abs(a).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"real count: {}\".format(sum(sum(eval_labels[index]))))\n",
    "# print(\"estimated_count: {}\".format(sum(sum(estimated))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
